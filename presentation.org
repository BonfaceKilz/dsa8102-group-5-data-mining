#+TITLE: Profanity and Political check in Twitter for website feeds (Group 5)
#+AUTHOR: Rose Kwamboka, Dasiy Mboya, Juma Matundura, Jeremy Gachanja, Antoinette Mutugi, Bonface M. K.

* Introduction

- Social Media is distracting.

- Use cases where social media is useful:
  - Dashboard of major conferences/ workshops (FOSDEM)
  - Hackathons
  - As part of a website

- Is there a way to filter content to remove non-relevant
  content(politics/ profane sentiments)?

Example Politic content:
- Trump won/ lost the elections

Not relevant for a feed that's supposed to show Scientific data!

(Ambitious) AIM: Present a WIP demo of the idea! And re-usable

* How does this project work?

- Pipelines: Cleaning the data(pre-processing), storage, later
  retrieval

 - 2 Parts:
   Data retrieval-- A daemon!
   Data Viewing-- A feed!

** Scraping

- Scraping happens every 10 seconds.

- We use [[https://github.com/twintproject/twint][Python-twint]]

- No API required. Max of about ~ 10K- 18k tweets mine-able!

- Open-source

** Pre-processing and filtering

- Remove stop-words
- Lemmatize-- Reduce words to core meaning .e.g. Thieves -> Thief
  (important for following step)
- Check for profanity and political sentiments. We use [[https://github.com/snguyenthanh/better_profanity][better_profanity]]
**** How it tied together
Chain of abstraction!
#+begin_src python
        # Get tweets(using twint)
        tweets = [get_tweets(user_name=user,
                             search_term=SEARCH_TERMS,
                             limit=5) for user in USERS.split(",")]
        
        for tweet in chain(*tweets):
            # Pre-process the tweet before checking for filter words!
            if ((not redis_conn.exists(f"tweets:{tweet.id_}")) and
                is_viewable_tweet(
                    lemmatize(remove_stopwords(tweet.tweet)))):
                _tweet = {
                    f"tweet:{tweet.id_}": {
                        "id": tweet.id_,
                        "tweet": tweet.tweet,
                        "user_name": tweet.user_name,
                        "date": tweet.date_,
                        "likes": tweet.likes,
                        "link": tweet.link,
                        "replies": tweet.replies,
                        "retweets": tweet.retweets}}
                # Actual Storage!
                with redis_conn.pipeline() as pipe:
                    for tweet_id, content in _tweet.items():
                        pipe.hmset(tweet_id, content)
                    pipe.execute()
                redis_conn.bgsave()
                redis_conn.expire(f"tweets:{tweet.id_}",
                                  timedelta(days=2))
        print("Done storing tweets!")
        time.sleep(10000)
#+end_src
** Storage

- Store data to REDIS using a daemon.

  Why Redis?

- Features for expiring data
- Light weight and open-source
- Easy to set-up
- Reduced code boiler-plate

** Display

- Data is displayed in a browser(demo)
- Same data could be displayed as part of another feed!

* Drawbacks

- Some words are anonymous like "resignation". False hits.
- How to rank tweets wrt popularity, say retweets and likes; and
  relevance
- Different formats of content from different spaces(like commits on
  public repos)!


* Code

See this [[https://github.com/BonfaceKilz/dsa8102-group-5-data-mining][link]]


* Future Work

- Use a robust text-classifying model for ranking(see BioSentVec(26
  GB!))

- Find a way to score tweets so that the most relevant data is
  displayed. Atm, only filtered out tweets are displayed. The list can
  get long!

- Fetch data from other social media platforms: Slack, IRC, Matrix,
  Fedi-verse, etc etc

- Aggregate data from code repositories

- Make this a library! And package it in GUIX (and Arch-Linux if usage
  is high)

- Port idea to other languages(like Scheme)!
